{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227579"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r', encoding = 'utf-8').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # making lowercase\n",
    "    text = text.lower()\n",
    "    # replacing all punctuation except dots with spaces\n",
    "    new_text = re.sub('[\\!\\?#$%\\(\\)\\*\\,\\-\\/\\:\\;\\=\\@\\[\\]\\—\\‘\\’\\“\\”]', ' ', text)\n",
    "    # collapsing multiple spaces into one '   ' -> ' '\n",
    "    clean_text = re.sub('\\s\\s+', ' ', new_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess_text(text)\n",
    "assert len(text) == 3141169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]\n",
    "text = list(filter(None, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        \n",
    "        # tokenizing text by symbols\n",
    "        self.itos = list(sorted(set(' '.join(text))))\n",
    "        for indx, i in enumerate(self.itos):\n",
    "            self.stoi[i] = indx\n",
    "            \n",
    "        encoded_text = []\n",
    "        \n",
    "        for line in text:\n",
    "            arr_line = []\n",
    "            \n",
    "            for i in line:\n",
    "                arr_line.append(self.stoi[i])\n",
    "            encoded_text.append(arr_line)\n",
    "            \n",
    "        text = encoded_text\n",
    "        \n",
    "        to_append = self.vocab_size - len(self.itos)\n",
    "        count = 0\n",
    "        \n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # counting bigram freqencies in the text\n",
    "            bigrams = Counter()\n",
    "            for i in text:\n",
    "                bigrams.update(zip(i, islice(i, 1, None)))\n",
    "            \n",
    "            for indx, i in enumerate(bigrams.most_common(to_append+1)):\n",
    "                if indx == count:\n",
    "                    new_token = bigrams.most_common(to_append+1)[indx][0]\n",
    "                    break\n",
    "\n",
    "            new_id = len(self.itos)\n",
    "            \n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = new_id\n",
    "    \n",
    "            count += 1\n",
    "            \n",
    "        # finding occurences of the new_token in the text and replace them with new_id\n",
    "        new_text = []\n",
    "\n",
    "        for line in text:\n",
    "            i = 0\n",
    "            arr = []\n",
    "            \n",
    "            while i < len(line):\n",
    "                if i == len(line) - 1:\n",
    "                    arr.append(line[i])\n",
    "                    \n",
    "                elif (line[i], line[i + 1]) in self.itos:\n",
    "                    tup = (line[i], line[i + 1])\n",
    "                    arr.append(self.stoi[tup])\n",
    "                    i += 1\n",
    "                    \n",
    "                else:\n",
    "                    arr.append(line[i])\n",
    "                    \n",
    "                i += 1\n",
    "                \n",
    "            new_text.append(arr)\n",
    "                    \n",
    "        text = new_text\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        encoded_text = []\n",
    "        \n",
    "        for line in text:\n",
    "            arr_line = []\n",
    "            \n",
    "            for i in line:\n",
    "                arr_line.append(self.stoi[i])\n",
    "            encoded_text.append(arr_line)\n",
    "            \n",
    "        text = encoded_text    \n",
    "        \n",
    "        to_append = self.vocab_size - len(self.itos)\n",
    "        count = 0\n",
    "        \n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # counting bigram freqencies in the text\n",
    "            bigrams = Counter()\n",
    "            for i in text:\n",
    "                bigrams.update(zip(i, islice(i, 1, None)))\n",
    "            \n",
    "            for indx, i in enumerate(bigrams.most_common(to_append+1)):\n",
    "                if indx == count:\n",
    "                    new_token = bigrams.most_common(to_append+1)[indx][0]\n",
    "                    break\n",
    "\n",
    "            new_id = len(self.itos)\n",
    "            \n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = new_id\n",
    "    \n",
    "            count += 1\n",
    "            \n",
    "        # finding occurences of the new_token in the text and replace them with new_id\n",
    "        new_text = []\n",
    "\n",
    "        for line in text:\n",
    "            i = 0\n",
    "            arr = []\n",
    "            \n",
    "            while i < len(line):\n",
    "                if i == len(line) - 1:\n",
    "                    arr.append(line[i])\n",
    "                    \n",
    "                elif (line[i], line[i + 1]) in self.itos:\n",
    "                    tup = (line[i], line[i + 1])\n",
    "                    arr.append(self.stoi[tup])\n",
    "                    i += 1\n",
    "                    \n",
    "                else:\n",
    "                    arr.append(line[i])\n",
    "                    \n",
    "                i += 1\n",
    "                \n",
    "            new_text.append(arr)\n",
    "                    \n",
    "        text = new_text\n",
    "        return text\n",
    "    \n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        \n",
    "        def search(token):\n",
    "            \n",
    "            if isinstance(token, int) == True:\n",
    "                t = self.itos[token]\n",
    "            \n",
    "                if isinstance(t, str) == True:\n",
    "                    return t\n",
    "            \n",
    "                else:\n",
    "                    return search(t)\n",
    "                \n",
    "            if isinstance(token, tuple) == True:\n",
    "                return str(search(token[0]) + search(token[1]))\n",
    "                    \n",
    "        return search(tok)\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))\n",
    "        \n",
    "        \n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "        \n",
    "    \n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "        \n",
    "    \n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.proba = Counter()\n",
    "        \n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        for token in range(self.vocab_size):\n",
    "            result.append(self.get_proba(a, b, token, tau))\n",
    "            \n",
    "        return np.array(result)\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        number = []\n",
    "        delta = 1\n",
    "        \n",
    "        for token in range(self.vocab_size):\n",
    "            smooth = (self.proba[(a, b, token)] + delta) ** (1 / tau)\n",
    "            number.append(smooth)\n",
    "            \n",
    "        result = ((self.proba[(a, b, c)] + delta) ** (1 / tau)) / sum(number)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        new = []\n",
    "        for line in text:\n",
    "            new.append([start_token] + line + [end_token])\n",
    "        \n",
    "        three = []\n",
    "        for line in new:\n",
    "            for i in range(len(line) - 2):\n",
    "                three.append((line[i], line[i + 1], line[i + 2]))\n",
    "        \n",
    "        self.proba = Counter(three)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "lm = LM(vocab_size, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(t):\n",
    "    \n",
    "    new = [start_token] + t + [end_token]\n",
    "    \n",
    "    three = []\n",
    "    \n",
    "    for i in range(len(new) - 2):\n",
    "        three.append((new[i], new[i + 1], new[i + 2]))\n",
    "            \n",
    "    proba = Counter(three)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.741592066940445e+40"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "def perplexity(snt, lm):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    proba = trigrams(snt)\n",
    "    p = 0\n",
    "    \n",
    "    for t in proba:\n",
    "        p += np.log(lm.get_proba(*t))\n",
    "    \n",
    "    result = exp(p) ** (-1/n)\n",
    "    return result\n",
    "\n",
    "perplexity(tokenized_text[0], lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "#бим серч не рабочий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    beam = [(input_seq, 1)]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        \n",
    "        for snt, snt_proba in beam:\n",
    "            if snt == end_token:\n",
    "                continue\n",
    "                \n",
    "            else:    \n",
    "                proba = lm.infer(snt[-2], snt[-1], tau)\n",
    "                best_k = sorted(enumerate(proba), key=lambda x:x[1], reverse=True)[:k]\n",
    "                \n",
    "                for token in best_k:\n",
    "                    candidates.append(snt + [token])\n",
    "                \n",
    "                for token, proba in enumerate(best_k):\n",
    "                    candidates_proba.append(snt_proba + np.log(proba))\n",
    "                \n",
    "        beam = [(candidates[i], candidates_proba[i]) for i in range(k)]\n",
    "        \n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 93, 30, 58, (34, 0.8547266510634597), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745), (0, 0.00980392156862745)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-744-586e9bc4ffb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-659-8f883281b76f>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mconvert\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mids\u001b[0m \u001b[0minto\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-659-8f883281b76f>\u001b[0m in \u001b[0;36mdecode_token\u001b[1;34m(self, tok)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-659-8f883281b76f>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "for i in result:\n",
    "    print(i[0])\n",
    "    print(bpe.decode(i[0]), i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
